import "./style.css";
import LanguageDetect from "languagedetect";
import loadAndPlotPopulation from "./Plots/population.js";
import loadAndPlotWords from "./Plots/WordsADay.js";
import loadAndPlotEnglishSpeakers from "./Plots/EnglishSpeakers.js";
import loadAndPlotAllWords from "./Plots/AIWords.js"
import {dateToYearFraction, P_AI} from "./functions.js";

document.addEventListener("DOMContentLoaded", () => {
    loadAndPlotWords();
    loadAndPlotEnglishSpeakers();
    loadAndPlotAllWords();
    loadAndPlotPopulation();
    const toggle = document.querySelector('.disclaimer-toggle');
    const content = document.querySelector('.disclaimer-content');

    toggle.addEventListener('click', function () {
        this.classList.toggle('active');

        if (content.style.display === 'block') {
            content.style.display = 'none';
        } else {
            content.style.display = 'block';
        }
    });
});

var today = new Date();
var dd = String(today.getDate()).padStart(2, '0');
var mm = String(today.getMonth() + 1).padStart(2, '0');
var yyyy = today.getFullYear();
today = dd + '/' + mm + '/' + yyyy;

var tomorrow = new Date();
tomorrow.setDate(tomorrow.getDate() + 1);
var dd_tom = String(tomorrow.getDate()).padStart(2, '0');
var mm_tom = String(tomorrow.getMonth() + 1).padStart(2, '0');
var yyyy_tom = tomorrow.getFullYear();
tomorrow = dd_tom + '/' + mm_tom + '/' + yyyy_tom;

document.querySelector("#title").innerHTML = `
    <h1>The World's Most Accurate* AI Text Detector</h1>
    <p class="read-the-docs">* Which really doesn't mean much</p>
`;

document.querySelector("#input").innerHTML = `
    <div class="input-container">
        <textarea id="text-input" rows="15" cols="100" placeholder="Enter at least 40 words of English text..." style="display: block; margin-bottom: 10px;"></textarea>
        <button id="check-button" style="display: block; margin: 0 auto;" disabled>Check for AI</button>
        <p id="error-message" style="color: red; display: none; text-align: center;"></p>
        <div id="result-container" style="display: none; margin-top: 20px; text-align: center;">
        <h3>Probability that this text is AI-generated is: <strong id="base-rate">0.00%</strong></h3>
        <h4>Tomorrow it will be: <strong id="tomorrow-rate">0.00%</strong></h4>
    </div>
    </div>
`;


document.querySelector("#article").innerHTML = `
    <div class="formula">
    $$P_{\\text{AI}}(t) = \\frac{\\int_{t_0}^{t} \\text{AI}(\\tau) \\, d\\tau}{\\int_{t_0}^{t} \\text{AI}(\\tau) \\, d\\tau + \\int_{t_0}^{t} W(\\tau) \\, d\\tau}$$
</div>
    <p>Where:</p>
    <ul>
        <li>$P_{AI}(t):$ Probability a random word is AI-generated by year</li>
        <li>$AI(t):$ AI-generated words per day at time $t$</li>
        <li>$W(t):$ Human-generated words per day at time $t$</li>
        <li>$t_0:$ Starting year for integration</li>
        <li id="today"></li>
    </ul>
    <div class="disclaimer-container">
      <button class="disclaimer-toggle">Disclaimer</button>
      <div class="disclaimer-content">
        <p>This is nothing more than a thought experiment and guesswork with many, many caveats that I have largely omitted, and while I am employed at a company that works primarily with Generative AI,
        I tried to be as objective as possible with my "estimations."</p>
        
        <p>That being said, the views expressed here are solely my own.</p>
      </div>
    </div>
    <p><b>What we are trying to do:</b> Estimating the probability that a piece of text is AI-generated by dividing the number of AI-generated words by the total number of words written by humans.</p>
    <p>In order to calculate the number of human-generated words until today, we will have to set some constraints:</p>
    <ol>
    <li>We are only considering English for simplicity.</li>
    <li>We are only considering the timeframe [23/4/1564, 1/1/2100]<span class="tooltip">[?]
    <span class="tooltiptext">23/4/1564 is the estimated birthdate of William Shakespeare, and I think it only fitting that the first time English (as we know it) was generated was when someone put pen to paper on the Bard’s birth certificate.
2100 seemed like a nice number with plenty of room for the relevant future, and with any luck, I won’t be around to see how inaccurate the prediction is by then
</span>
  </span></li>
    <li>We will only count individual texts, a book of 100 thousand words counts as 100 thousand words no matter how often it has been printed</li>
    <li>For the estimation of AI text, we are constraining ourselves to text that humans realistically have access to (otherwise, why put it in an AI detector)</li>
</ol>
    <h2>Part I: Human-Generated Words $W(t)$</h2>
    <p>The first part of this project called for an estimation of the total number of English words that have ever been generated by humans.
    This is a daunting task in and of itself, but somewhat manageable if we constrain the problem and break it down into smaller problems.</p>

    <div class="formula">$$W(t) = H(t) \\times E(t) \\times G(t)$$</div>
    <p>Where:</p>
    <ul>
        <li>$H(t):$ Total number of humans</li>
        <li>$E(t):$ Fraction of the human population speaking English</li>
        <li>$G(t):$ Number of words generated by the average literate person</li>
    </ul>
    <h3>1. Human Population $H(t)$</h3>
    <p>Again, a daunting task, but constraint number 2 comes in very handy here. We have historical data of the human population and
    are “only” trying to model the population growth until 2100. This has been done and will be more accurate than any model I can come up with.</p>
    <p><a href="https://www.worldometers.info/world-population/world-population-projections/">This website</a> gives us some projections for population growth up to 2100.
    And <a href="https://ourworldindata.org/grapher/population?time=1500..latest">this website</a> gives us data for the 1500s until today.
    Using cubic spline interpolation, we can get a reasonable estimate for the human population function.</p>
<canvas id="populationChart"></canvas>
    <h3>2. Fraction of Literate English Speakers $E(t)$</h3>
    <p>This one is a little bit trickier and we can not fully rely on past data.
The underlying function that seems to make the most sense is a logistic growth function:</p>
    <div class="formula">$$E(t) = \\frac{E_{\\text{max}}}{1 + e^{-k_E (t - t_E)}}$$</div>
<p>It <a href="https://www.statista.com/statistics/266808/the-most-spoken-languages-worldwide/">is estimated</a> that in 2025 the fraction of the human population speaking English is ~0.18, so we can set E(2025)=0.18.
<a href="https://wordsrated.com/how-many-people-speak-english/">Another data source</a> cites 5 recent years.
There are conflicting data sources however, an <a href="https://www.science.org/doi/10.1126/science.1096546">article</a> published in Science in 2004 gave a pessimistic view of the development of the future of the English language.
Putting the percentage of the world population speaking English on a steady and fairly rapid decline since 1950.</p>
<p>While I do think the fraction of the native English speaking population has decreased due to the rapid growth of other countries
and the stagnating growth of majority English countries, the author could not predict the expansion of English as the Lingua Franca of large portions of the internet.
However we will take up the data point of 1950 with 0.09 with an additional 10% for non-native speakers.</p>
<p>This is a very narrow range to estimate the rest of the parameters, but it’s better than nothing. Now to fit the parameters of the logistic growth:</p>
<ul>
<li>It is unlikely that English will achieve full or even majority dominance on the planet. Majority English speaking countries have mostly reached their saturation limit of population and already only 25% of English speakers are natives.</li>
<li>Language adoption is rather slow, meaning our growth rate should be set to somewhere between 0.01 and 0.05</li>
<li>The inflection point of English speaking will still likely be set in the future, as English education and adoption very gradually improves in non-English majority countries.</li>
</ul>
<p>Fidgeting with the parameters to roughly fit our data we get the following:
$$E_{\\text{max}} = 0.33, \\quad k_E = 0.014, \\quad t_E = 2016$$
</p>
<canvas id="speakersChart"></canvas>
    <h3>3. Words per Person $G(t)$</h3>
    <p>If you thought the last section contained only glorified guesswork, you ain’t seen nothing yet. 
    How do we quantify the number of words the average human writes per day? <a href="https://www.youtube.com/watch?v=m8niIHChc1Y">What even is a word?</a></p>
    <p>To generate the data, I asked different reasoning LLMs to come up with an estimate and put the average data points in here. They came up with remarkably similar numbers. So we will just take their average.</p>
     <table>
  <tr>
    <th>Year</th>
    <th>Average Words</th>
    <th>Reasoning</th>
  </tr>
  <tr>
    <td>1700</td>
    <td>~25</td>
    <td>Low literacy, very few books, pamphlets, etc. published</td>
  </tr>
  <tr>
    <td>1900</td>
    <td>~200</td>
    <td>Rise in literacy, expanded urbanization and education</td>
  </tr>
    <td>2000</td>
    <td>~1000</td>
    <td>Large shift towards digital written communication</td>
  </tr>
    <td>2025</td>
    <td>~2000</td>
    <td>Social media, remote work, instant messaging, and near universal reachability</td>
  </tr>
</table>
<p>We will also need to consider the reduction in human-generated words due to AI after 2024 and put a (rather significant) dampener on the growth rate.
However, the current interaction with AI largely relies on written prompting, maybe roughly offsetting what AI replaces with the words it takes to get it to replace text.
In the future, we might replace written prompting with voice interaction once it becomes more conventient and the tools more sophisticated.
This, after all, is what we previously imagined AI interaction to look like.</p>
    <div class="formula">$$G(t) = \\left( \\frac{G_{\\text{max}}}{1 + e^{-k_G (t - t_G)}} + 15 \\right) \\times \\left(1 - r_{\\text{AI}}\\right)^{\\max(0, t - t_{\\text{AI}})}$$
<p>
Where:
$$G_{\\text{max}} = 4000, \\quad k_G = 0.03, \\quad t_G = 2000$$
$$r_{\\text{AI}} = 0.008, \\quad t_{\\text{AI}} = 2024$$
</p>
</div>
<canvas id="wordsChart"></canvas>
<p>We can see that the logistic growth function might not be the perfect fit for this model, not catching the rapid rise of
social media, and other distinct "spikes" in technology that enable connectivity, etc. But it will probably more or less average out and we are not aiming for perfection anyway.</p>
    <h2>Part II: AI-Generated Words $AI(t)$</h2>
    <p>Now that we have the human-generated words, we can estimate the AI-generated words.
    This is a bit trickier, as we need to consider the different types of AI text that exist and we have almost no established data to base our estimates on.
    Again though, we will try and break the problem into smaller ones that we will breeze through more quickly:</p>
    <div class="formula">
    $$AI(t) = \\text{AllText}(t) \\times \\text{EngShare}(t) \\times \\text{TxtShare}(t)$$
</div>

<h3>1. Total Visible AI Text $\\text{AllText}(t)$</h3>
<p>This we can approximate similar to the adoption of other technologies that we now see commonplace,
setting the asymptotic maximum at a reasonable value of text that humans can realistically consume.</p>
<p>With the rampant hype and absurd amounts of money being poured into AI development, with a new "revolutionary" model
coming out every week, we might reasonably assume that the inflection point of AI text generation is not that far in the future.
However, AI innovation does obviously not equal AI adoption, but it is hard to imagine that the current pace of the hype will continue for long.</p>
<p>The near future will most likely contain a more quiet increase in AI use in sectors where it is useful (and let's be honest, sectors where it really isn't).</p>
<div class="formula">
  $$\\text{AllText}(t) = \\frac{W_{\\text{max}}}{1 + e^{-k(t - t_{\\text{mid}})}} $$
  <p>
    Where:
    $$W_{\\text{max}} = 3.0 \\times 10^{12}, \\quad k = 2.1, \\quad t_{\\text{mid}} = 2025.5 $$
  </p>
</div>
    <h3>2. English Language Share $\\text{EngShare}(t)$</h3>
    <p>This is just a simple decay function, modeling the reduction in English AI text over time.</p>
    <div class="formula">
    $$\\text{EngShare}(t) = \\max\\left( E_{\\text{floor}},\\ E_{\\text{start}} - r_E (t - 2023) \\right)$$
<p>
Where:
$$E_{\\text{start}} = 0.80, \\quad r_E = 0.02, \\quad E_{\\text{floor}} = 0.50$$
</p>
</div>
    <h3>3. Text Modality Share $\\text{TxtShare}(t)$</h3>
<p>Another simple decay function that reduces the total text produced by AI considering the expected modality shift towards more information-dense spoken language and video content.</p>
    <div class="formula">$$\\text{TxtShare}(t) = \\max\\left( T_{\\text{floor}},\\ 1 - r_T \\cdot \\max(0,\\ t - t_T) \\right)$$
<p>
Where:
$$t_T = 2028, \\quad r_T = 0.01, \\quad T_{\\text{floor}} = 0.60$$
</p>
</div>
<canvas id="AIWords"></canvas>
Now there are of course a million caveats, assumptions, and so much more complexity in this estimation than we could ever cover,
but I believe the general idea is more or less sound. 
<h2>Part III: Putting it in Perspective</h2>
<canvas id="combined"></canvas>
<p>While we will have to wait and see what the future holds, even the rather optimistic estimation of two trillion AI-generated words in 2030 pales in comparison 
to what our models say humans will output.</p>
<p>This brings me to the point of this write-up: When we wade through a swamp of AI news articles and TikTok slop,
remember that the way out is for you to write and create. Use this technology as the enabling tool that it is supposed to be: learn with it, question it, and
use it to your advantage, don't use it to write everything you could also write. These are incredibly sophisticated tools that can enable us to do remarkable things.</p>
<div class="article_footer" id="article_footer"></div>
`;

const lngDetector = new LanguageDetect();
const textInput = document.getElementById("text-input");
const errorMessage = document.getElementById("error-message");
const checkButton = document.getElementById("check-button");

textInput.addEventListener("input", validateInput);

function validateInput() {
    const text = textInput.value.trim();
    const wordCount = text.split(/\s+/).filter(word => word.length > 0).length;
    let error = "";
    let isError = false;

    if (wordCount < 40 && text.length > 0) {
        error = "Minimum 40 words required";
        isError = true;
    }

    if (!isError && wordCount >= 40) {
        const detectedLanguage = lngDetector.detect(text, 1);
        if (detectedLanguage.length === 0 || detectedLanguage[0][0] !== "english") {
            error = "Only English text is supported";
            isError = true;
        }
    }

    if (isError) {
        showError(error);
        disableButton();
    } else {
        hideError();
        enableButton();
    }

    if (text.length === 0) {
        disableButton();
    }
}

function showError(message) {
    textInput.style.border = "2px solid red";
    errorMessage.style.display = "block";
    errorMessage.textContent = message;
}

function hideError() {
    textInput.style.border = "";
    errorMessage.style.display = "none";
    errorMessage.textContent = "";
}

function disableButton() {
    checkButton.disabled = true;
    checkButton.style.opacity = "0.5";
}

function enableButton() {
    checkButton.disabled = false;
    checkButton.style.opacity = "1";
}

checkButton.addEventListener("click", function () {
    checkButton.textContent = "Calculating...";
    checkButton.disabled = true;
    setTimeout(() => {
        try {
            const P = P_AI(dateToYearFraction(today));
            const baseRatePercent = (P * 100).toFixed(2);
            const P_tmrw = P_AI(dateToYearFraction(tomorrow));
            const tmrwRatePercent = (P_tmrw * 100).toFixed(2);
            document.getElementById("base-rate").textContent = `${baseRatePercent}%`;
            document.getElementById("tomorrow-rate").textContent = `${tmrwRatePercent}%`;
            document.getElementById("result-container").style.display = "block";
        } catch (error) {
            console.error("Error calculating AI probability:", error);
            showError("An error occurred during calculation. Please try again.");
        } finally {
            checkButton.textContent = "Check for AI";
            checkButton.disabled = false;
        }
    }, 100);
});


document.getElementById("today").innerHTML = "$t:$ Current date (" + today + ")";
document.getElementById("article_footer").innerHTML = "Copyright © " + yyyy + " Sebastian Oßner";
